{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "Cuda_id:  0\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "print('Cuda available:', torch.cuda.is_available())\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print('Cuda_id: ', cuda_id)\n",
    "print(torch.cuda.get_device_name(cuda_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file backup/df_output_SP.pkl exist and will returned\n",
      "The file backup/df_output_EN.pkl exist and will returned\n"
     ]
    }
   ],
   "source": [
    "#%% Data Loading\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "file_path = 'backup/df_output_SP.pkl'\n",
    "def save_data(file_path, data):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"The file {file_path} exist and will returned\")\n",
    "        data = pd.read_pickle(file_path)\n",
    "        return data\n",
    "    else: \n",
    "        data.to_pickle(file_path)\n",
    "        print(f\"Data has been saved to {file_path}.\")\n",
    "        return data\n",
    "\n",
    "df_output_SP = save_data('backup/df_output_SP.pkl', None)\n",
    "df_output_EN = save_data('backup/df_output_EN.pkl', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output_EN = df_output_EN.sample(n=700, random_state=42)\n",
    "df_output_SP = df_output_SP.sample(n=700, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinct-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Distinct-1 score for the df_output_SP SIM: 0.9345773934589631\n",
      "Average Distinct-1 score for the df_output_SP MAP: 0.9189624512900665\n",
      "Average Distinct-1 score for the df_output_EN SIM: 0.9463274876485652\n",
      "Average Distinct-1 score for the df_output_EN MAP: 0.9445213093654956\n",
      "Average Distinct-2 score for the df_output_SP SIM: 0.9248617945026216\n",
      "Average Distinct-2 score for the df_output_SP MAP: 0.9330922492114592\n",
      "Average Distinct-2 score for the df_output_EN SIM: 0.9047808834668024\n",
      "Average Distinct-2 score for the df_output_EN MAP: 0.8901043383056046\n"
     ]
    }
   ],
   "source": [
    "#%% distinct_n\n",
    "from distinct_n.metrics import distinct_n_sentence_level, distinct_n_corpus_level\n",
    "\n",
    "n = 1  # For Monograms\n",
    "\n",
    "corpus = list(df_output_SP['output_SIM'])\n",
    "score = distinct_n_corpus_level([sentence.split() for sentence in corpus], n)\n",
    "print(f\"Average Distinct-{n} score for the df_output_SP SIM: {score}\")\n",
    "\n",
    "corpus = list(df_output_SP['output_MAP'])\n",
    "score = distinct_n_corpus_level([sentence.split() for sentence in corpus], n)\n",
    "print(f\"Average Distinct-{n} score for the df_output_SP MAP: {score}\")\n",
    "\n",
    "corpus = list(df_output_EN['output_SIM'])\n",
    "score = distinct_n_corpus_level([sentence.split() for sentence in corpus], n)\n",
    "print(f\"Average Distinct-{n} score for the df_output_EN SIM: {score}\")\n",
    "\n",
    "corpus = list(df_output_EN['output_MAP'])\n",
    "score = distinct_n_corpus_level([sentence.split() for sentence in corpus], n)\n",
    "print(f\"Average Distinct-{n} score for the df_output_EN MAP: {score}\")\n",
    "\n",
    "n = 2  # For bigrams\n",
    "\n",
    "corpus = list(df_output_SP['output_SIM'])\n",
    "score = distinct_n_corpus_level([sentence.split() for sentence in corpus], n)\n",
    "print(f\"Average Distinct-{n} score for the df_output_SP SIM: {score}\")\n",
    "\n",
    "corpus = list(df_output_SP['output_MAP'])\n",
    "score = distinct_n_corpus_level([sentence.split() for sentence in corpus], n)\n",
    "print(f\"Average Distinct-{n} score for the df_output_SP MAP: {score}\")\n",
    "\n",
    "corpus = list(df_output_EN['output_SIM'])\n",
    "score = distinct_n_corpus_level([sentence.split() for sentence in corpus], n)\n",
    "print(f\"Average Distinct-{n} score for the df_output_EN SIM: {score}\")\n",
    "\n",
    "corpus = list(df_output_EN['output_MAP'])\n",
    "score = distinct_n_corpus_level([sentence.split() for sentence in corpus], n)\n",
    "print(f\"Average Distinct-{n} score for the df_output_EN MAP: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ent-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy-1 for the df_output_SP MAP: 8.918680497118206\n",
      "Entropy-1 for the df_output_SP SIM: 8.40171141669043\n",
      "Entropy-1 for the df_output_EN MAP: 9.3317688882827\n",
      "Entropy-1 for the df_output_EN SIM: 9.07602335936366\n",
      "Entropy-2 for the df_output_SP MAP: 12.220774474826445\n",
      "Entropy-2 for the df_output_SP SIM: 11.473927504071174\n",
      "Entropy-2 for the df_output_EN MAP: 12.610284421048435\n",
      "Entropy-2 for the df_output_EN SIM: 12.075933281349133\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_entropy_corpus(corpus, n):\n",
    "    # Combine all sentences into one list of words\n",
    "    combined_words = []\n",
    "    for sentence in corpus:\n",
    "        combined_words.extend(sentence.split())\n",
    "    \n",
    "    # Generate n-grams\n",
    "    ngrams = [tuple(combined_words[i:i+n]) for i in range(len(combined_words) - n + 1)]\n",
    "    \n",
    "    # Count the frequency of each n-gram\n",
    "    ngram_freq = Counter(ngrams)\n",
    "    \n",
    "    # Calculate the total number of n-grams\n",
    "    total_ngrams = sum(ngram_freq.values())\n",
    "    \n",
    "    # Calculate the probability of each n-gram\n",
    "    ngram_probs = {ngram: freq / total_ngrams for ngram, freq in ngram_freq.items()}\n",
    "    \n",
    "    # Calculate the entropy\n",
    "    entropy = -sum(prob * math.log(prob, 2) for prob in ngram_probs.values())\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "n = 1\n",
    "\n",
    "corpus = list(df_output_SP['output_MAP'])\n",
    "entropy = calculate_entropy_corpus(corpus, n)\n",
    "print(f\"Entropy-{n} for the df_output_SP MAP: {entropy}\")\n",
    "\n",
    "corpus = list(df_output_SP['output_SIM'])\n",
    "entropy = calculate_entropy_corpus(corpus, n)\n",
    "print(f\"Entropy-{n} for the df_output_SP SIM: {entropy}\")\n",
    "\n",
    "corpus = list(df_output_EN['output_MAP'])\n",
    "entropy = calculate_entropy_corpus(corpus, n)\n",
    "print(f\"Entropy-{n} for the df_output_EN MAP: {entropy}\")\n",
    "\n",
    "corpus = list(df_output_EN['output_SIM'])\n",
    "entropy = calculate_entropy_corpus(corpus, n)\n",
    "print(f\"Entropy-{n} for the df_output_EN SIM: {entropy}\")\n",
    "\n",
    "n = 2\n",
    "\n",
    "corpus = list(df_output_SP['output_MAP'])\n",
    "entropy = calculate_entropy_corpus(corpus, n)\n",
    "print(f\"Entropy-{n} for the df_output_SP MAP: {entropy}\")\n",
    "\n",
    "corpus = list(df_output_SP['output_SIM'])\n",
    "entropy = calculate_entropy_corpus(corpus, n)\n",
    "print(f\"Entropy-{n} for the df_output_SP SIM: {entropy}\")\n",
    "\n",
    "corpus = list(df_output_EN['output_MAP'])\n",
    "entropy = calculate_entropy_corpus(corpus, n)\n",
    "print(f\"Entropy-{n} for the df_output_EN MAP: {entropy}\")\n",
    "\n",
    "corpus = list(df_output_EN['output_SIM'])\n",
    "entropy = calculate_entropy_corpus(corpus, n)\n",
    "print(f\"Entropy-{n} for the df_output_EN SIM: {entropy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge-2 for the df_output_SP SIM: 0.060004415053185024\n",
      "Rouge-2 for the df_output_SP MAP: 0.07194993280934014\n",
      "Rouge-2 for the df_output_EN SIM: 0.0035095042297832186\n",
      "Rouge-2 for the df_output_EN MAP: 0.0033139187237304145\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Function to calculate ROUGE-2 score for each row\n",
    "scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
    "\n",
    "def calculate_rouge(row):\n",
    "    scores = scorer.score(row['reference'], row['generated'])\n",
    "    return scores['rouge2'].fmeasure\n",
    "\n",
    "n = 2\n",
    "df = pd.DataFrame({'reference': list(df_output_SP['y_text']), 'generated': list(df_output_SP['output_SIM'])})\n",
    "df['rouge2'] = df.apply(calculate_rouge, axis=1)\n",
    "print(f\"Rouge-{n} for the df_output_SP SIM: {np.mean(df['rouge2'])}\")\n",
    "\n",
    "df = pd.DataFrame({'reference': list(df_output_SP['y_text']), 'generated': list(df_output_SP['output_MAP'])})\n",
    "df['rouge2'] = df.apply(calculate_rouge, axis=1)\n",
    "print(f\"Rouge-{n} for the df_output_SP MAP: {np.mean(df['rouge2'])}\")\n",
    "\n",
    "df = pd.DataFrame({'reference': list(df_output_EN['y_text']), 'generated': list(df_output_EN['output_SIM'])})\n",
    "df['rouge2'] = df.apply(calculate_rouge, axis=1)\n",
    "print(f\"Rouge-{n} for the df_output_EN SIM: {np.mean(df['rouge2'])}\")\n",
    "\n",
    "df = pd.DataFrame({'reference': list(df_output_EN['y_text']), 'generated': list(df_output_EN['output_MAP'])})\n",
    "df['rouge2'] = df.apply(calculate_rouge, axis=1)\n",
    "print(f\"Rouge-{n} for the df_output_EN MAP: {np.mean(df['rouge2'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu for the df_output_SP SIM: 0.031695689377054345\n",
      "Bleu for the df_output_SP MAP: 0.045401016044072505\n",
      "Bleu for the df_output_EN SIM: 0.007081544131689892\n",
      "Bleu for the df_output_EN MAP: 0.006494955602031048\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Function to calculate BLEU score for each row\n",
    "def calculate_bleu(row):\n",
    "    reference = [row['reference'].split()]  # BLEU expects a list of reference lists\n",
    "    generated = row['generated'].split()\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    score = sentence_bleu(reference, generated, smoothing_function=smoothing_function)\n",
    "    return score\n",
    "\n",
    "df = pd.DataFrame({'reference': list(df_output_SP['y_text']), 'generated': list(df_output_SP['output_SIM'])})\n",
    "df['Bleu'] = df.apply(calculate_bleu, axis=1)\n",
    "print(f\"Bleu for the df_output_SP SIM: {np.mean(df['Bleu'])}\")\n",
    "\n",
    "df = pd.DataFrame({'reference': list(df_output_SP['y_text']), 'generated': list(df_output_SP['output_MAP'])})\n",
    "df['Bleu'] = df.apply(calculate_bleu, axis=1)\n",
    "print(f\"Bleu for the df_output_SP MAP: {np.mean(df['Bleu'])}\")\n",
    "\n",
    "df = pd.DataFrame({'reference': list(df_output_EN['y_text']), 'generated': list(df_output_EN['output_SIM'])})\n",
    "df['Bleu'] = df.apply(calculate_bleu, axis=1)\n",
    "print(f\"Bleu for the df_output_EN SIM: {np.mean(df['Bleu'])}\")\n",
    "\n",
    "df = pd.DataFrame({'reference': list(df_output_EN['y_text']), 'generated': list(df_output_EN['output_MAP'])})\n",
    "df['Bleu'] = df.apply(calculate_bleu, axis=1)\n",
    "print(f\"Bleu for the df_output_EN MAP: {np.mean(df['Bleu'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimelicea/miniconda3/envs/gps_metrics/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "# Initialize tqdm for pandas apply \n",
    "tqdm.pandas(desc=\"Processing rows\")\n",
    "\n",
    "import warnings \n",
    "from transformers import logging\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bertscore(row):\n",
    "    reference = [row['reference']]\n",
    "    generated = [row['generated']]\n",
    "    precision, recall, f1 = score(generated, reference, lang='es')\n",
    "    return f1.mean().item()  # Use .item() to get the value from the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimelicea/miniconda3/envs/gps_metrics/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu for the df_output_SP SIM: 0.7065473560776029\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'reference': list(df_output_SP['y_text']), 'generated': list(df_output_SP['output_SIM'])})\n",
    "df['BERTScore'] = df.apply(calculate_bertscore, axis=1)\n",
    "print(f\"BERTScore for the df_output_SP SIM: {np.mean(df['BERTScore'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimelicea/miniconda3/envs/gps_metrics/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore for the df_output_SP MAP: 0.7068764520706711\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'reference': list(df_output_SP['y_text']), 'generated': list(df_output_SP['output_MAP'])})\n",
    "df['BERTScore'] = df.apply(calculate_bertscore, axis=1)\n",
    "print(f\"BERTScore for the df_output_SP MAP: {np.mean(df['BERTScore'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bertscore(row):\n",
    "    reference = [row['reference']]\n",
    "    generated = [row['generated']]\n",
    "    precision, recall, f1 = score(generated, reference, lang='en')\n",
    "    return f1.mean().item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bert_score import score\n",
    "\n",
    "# Assuming calculate_bertscore is already defined\n",
    "def calculate_bertscore(reference, generated):\n",
    "    precision, recall, f1 = score([generated], [reference], lang='en')\n",
    "    return f1.mean().item()  # Use .item() to get the value from the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 700/700 [28:13<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore for the df_output_EN SIM: 0.841197082230023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'reference': list(df_output_EN['y_text']), 'generated': list(df_output_EN['output_SIM'])})\n",
    "df['BERTScore'] = 0.0\n",
    "\n",
    "# Use tqdm with a loop for progress visualization\n",
    "for i in tqdm(range(len(df)), desc=\"Processing rows\"):\n",
    "    df.at[i, 'BERTScore'] = calculate_bertscore(df.at[i, 'reference'], df.at[i, 'generated'])\n",
    "\n",
    "# Print the mean BERTScore\n",
    "print(f\"BERTScore for the df_output_EN SIM: {np.mean(df['BERTScore'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 700/700 [28:14<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore for the df_output_EN MAP: 0.835725862128394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'reference': list(df_output_EN['y_text']), 'generated': list(df_output_EN['output_MAP'])})\n",
    "df['BERTScore'] = 0.0\n",
    "\n",
    "# Use tqdm with a loop for progress visualization\n",
    "for i in tqdm(range(len(df)), desc=\"Processing rows\"):\n",
    "    df.at[i, 'BERTScore'] = calculate_bertscore(df.at[i, 'reference'], df.at[i, 'generated'])\n",
    "\n",
    "# Print the mean BERTScore\n",
    "print(f\"BERTScore for the df_output_EN MAP: {np.mean(df['BERTScore'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimelicea/miniconda3/envs/gps_metrics/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jaimelicea/miniconda3/envs/gps_metrics/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "config_class, model_class, tokenizer_class = BertConfig, BertForSequenceClassification, BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "saved_pretrained_CoLA_model_dir = \"./model_esCola\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ftt = 'EsCoLA' # 'EsCoLA' 'CoLA'\n",
    "\n",
    "config = config_class.from_pretrained(saved_pretrained_CoLA_model_dir, num_labels=2, finetuning_task=ftt)\n",
    "tokenizer = BertTokenizer.from_pretrained(saved_pretrained_CoLA_model_dir, do_lower_case=False)\n",
    "model = BertForSequenceClassification.from_pretrained(saved_pretrained_CoLA_model_dir, from_tf=bool('.ckpt' in model_name), config=config).to(device)\n",
    "\n",
    "#%% gruen_score\n",
    "def gruen_score(text, tokenizer, model, device):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        grammaticality_score = predictions.item()\n",
    "    return grammaticality_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Processing GRUEN scores\")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GRUEN scores: 100%|██████████| 700/700 [00:03<00:00, 191.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gruen_score for the df_output_SP SIM: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GRUEN scores: 100%|██████████| 700/700 [00:05<00:00, 133.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gruen_score for the df_output_SP MAP: 0.9671428571428572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_output_SP['GRUEN_score'] = df_output_SP['output_SIM'].progress_apply(lambda x: gruen_score(x, tokenizer, model, device))\n",
    "print(f\"gruen_score for the df_output_SP SIM: {np.mean(df_output_SP['GRUEN_score'])}\")\n",
    "\n",
    "df_output_SP['GRUEN_score'] = df_output_SP['output_MAP'].progress_apply(lambda x: gruen_score(x, tokenizer, model, device))\n",
    "print(f\"gruen_score for the df_output_SP MAP: {np.mean(df_output_SP['GRUEN_score'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaimelicea/miniconda3/envs/gps_metrics/lib/python3.9/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "saved_pretrained_CoLA_model_dir = \"./tmp/grammar_cola\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ftt = 'CoLA' # 'EsCoLA' 'CoLA'\n",
    "\n",
    "config = config_class.from_pretrained(saved_pretrained_CoLA_model_dir, num_labels=2, finetuning_task=ftt)\n",
    "tokenizer = BertTokenizer.from_pretrained(saved_pretrained_CoLA_model_dir, do_lower_case=False)\n",
    "model = BertForSequenceClassification.from_pretrained(saved_pretrained_CoLA_model_dir, from_tf=bool('.ckpt' in model_name), config=config).to(device)\n",
    "\n",
    "#%% gruen_score\n",
    "def gruen_score(text, tokenizer, model, device):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        grammaticality_score = predictions.item()\n",
    "    return grammaticality_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GRUEN scores: 100%|██████████| 700/700 [00:05<00:00, 126.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gruen_score for the df_output_EN SIM: 0.8628571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GRUEN scores: 100%|██████████| 700/700 [00:04<00:00, 144.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gruen_score for the df_output_EN MAP: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_output_EN['GRUEN_score'] = df_output_EN['output_SIM'].progress_apply(lambda x: gruen_score(x, tokenizer, model, device))\n",
    "print(f\"gruen_score for the df_output_EN SIM: {np.mean(df_output_EN['GRUEN_score'])}\")\n",
    "\n",
    "df_output_EN['GRUEN_score'] = df_output_EN['output_MAP'].progress_apply(lambda x: gruen_score(x, tokenizer, model, device))\n",
    "print(f\"gruen_score for the df_output_EN MAP: {np.mean(df_output_EN['GRUEN_score'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output_EN.rename(columns={'x_text': 'Text', 'output_SIM': 'SIM', 'output_MAP': 'MaP'}, inplace=True)\n",
    "df_output_EN[['Text','SIM', 'MaP']][:5].to_clipboard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output_SP.rename(columns={'x_text': 'Text', 'output_SIM': 'SIM', 'output_MAP': 'MaP'}, inplace=True)\n",
    "df_output_SP[['Text','SIM', 'MaP']][:5].to_clipboard(encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gps_metrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
